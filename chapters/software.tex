\chapter{Crisis and Systems, 1960-1980ish}

\todo{shit got crazy; became important, needs more focus/attention.}

\section{The Software Crisis}
\begin{quotation}
Despite great strides in software, programming always seemed to be in a state 
of crisis and always seemed to play catch-up to the advances in hardware. This 
crisis came to a head in 1968, just as the integrated circuit and disk storage 
were making their impact on hardware systems. That year, the crisis was 
explicitly acknowledged in the academic and trade literature and was the 
subject of a NATO-sponsored conference that called further attention to it. 
Some of the solutions proposed were a new discipline of software engineering, 
more formal techniques of structured programming, and new programming languages 
that would replace the venerable but obsolete COBOL and FORTRAN. Although not 
made in response to this crisis, the decision by IBM to sell its software and 
services separately from its hardware probably did even more to address the 
problem. It led to a commercial software industry that needed to produce 
reliable software in order to survive. The crisis remained, however, and became 
a permanent aspect of computing. Software came of age in 1968; the following 
decades would see further changes and further adaptations to hardware advances.
\cite{history_of_modern_computing_2003_ceruzzi}
\end{quotation}

\todo{DEC PDP-8 and PDP-11; IBM System/360 and OS/360; Multics; Unix; C}

As the 1960s progressed, the notion of \textit{software} became more established,
and the programs being written served the authors to a much greater extent.
Prior to the 1960s, programs were often tailor-made for a specific machine.
There was no hope of re-using the program on another machine.
For most of the users, their organization had spent a considerable portion
of their budget on their system, and they were expected to use it for a long time.
Retargetable compilers did not exist.

Michael Mahoney made a strong statement to this end rather early on \cite[The Structures of Computation]{the-first-computers-2002}:
\begin{quotation}
The kinds of computers we have designed since 1945 and the kinds of programs we 
have written for them reflect not the nature of the computer but the purposes 
and aspirations of the groups of people who made those designs and wrote those 
programs, and the product of their work reflects not the history of the 
computer but the histories of those groups, even as the computer in many cases 
fundamentally redirected the course of those histories.
\end{quotation}

\begin{quotation}
Despite great strides in software, programming always seemed to be in a state 
of crisis and always seemed to play catch-up to the advances in hardware. This 
crisis came to a head in 1968, just as the integrated circuit and disk storage 
were making their impact on hardware systems. That year, the crisis was 
explicitly acknowledged in the academic and trade literature and was the 
subject of a NATO-sponsored conference that called further attention to it. 
Some of the solutions proposed were a new discipline of software engineering, 
more formal techniques of structured programming, and new programming languages 
that would replace the venerable but obsolete COBOL and FORTRAN. Although not 
made in response to this crisis, the decision by IBM to sell its software and 
services separately from its hardware probably did even more to address the 
problem. It led to a commercial software industry that needed to produce 
reliable software in order to survive. The crisis remained, however, and became 
a permanent aspect of computing. Software came of age in 1968; the following 
decades would see further changes and further adaptations to hardware advances.
\end{quotation}

\todo{1945 was too early for this strong of a statement;
how can one argue that software reflected the authors when it was so dependent on the hardware?}

\section{Aho, Ullman, and Bell Labs}

\todo{Software (and compilers!) starts to become a real discipline!
Ullman was older and further along than Aho, and Hopcraft came to Princeton and became Aho's advisor.}

\begin{quotation}
One of the first people that I met at Princeton was a Columbia graduate by the 
name of Jeffrey Ullman. He had just gotten his undergraduate degree from 
Columbia University and also had come to study digital systems in the EE 
department at Princeton. So, he and I became close friends. When we graduated 
from Princeton, we both joined the newly formed Computing Sciences Research 
Center at Bell Labs. There we developed a lifelong collaboration on subjects 
ranging from algorithms, programming languages, to the very foundations of 
computer science. I was very fortunate to have met some of the greatest people 
in the field and to have gotten to know them and work with them. You learn so 
much by working with the best people in the field. So, I felt very blessed 
because I had this kind of background\dots

Hsu: Before we jump into Bell Labs more deeply, could you maybe explain-- talk about your PhD thesis,
but try to explain it to somebody who, maybe like a museum goer who doesn't really know much about
computer science and linguistics.

Aho: This is interesting. As I mentioned, Hopcroft told me, "Find your own research problem." He did
teach a course in automata and language theory, so I got introduced to formal language theory and
automata theory, at least, as it was known at that time. I was interested in programming languages and
compilers. What I noticed was that a programming language has a syntax and a semantics. All languages
have a syntax and a semantics. If you want to write a translator for a programming language, or even a
natural language, you have to understand the syntax and semantics of your source language and the
target language\dots

Hansen: 1967, and you followed Ullman there. He had already joined Bell Labs before.
Aho: A few months before me.
Hansen: A few months before. And what group was it that you joined?
Aho: I was interviewed by a department head by the name of Doug McIlroy. He was an applied
mathematician from MIT. He had been at Bell Labs for a few years before me. Amongst other things, he
had coinvented macros for programming languages and he's also in this class of one of the smartest
people I've ever met. 

Jeff wanted to go to academia a little bit earlier than I did, like many years 
earlier. He stayed at Bell Labs for a few years and went to Princeton 
University where he joined the faculty of the electrical engineering 
department, but he would come and spend one day a week consulting at Bell Labs. 
His consulting stint was he would come Fridays and sit in my office all day. 
The conversations that we'd have would range over all sorts of topics, and 
sometimes he'd mentioned that he was working on a problem with a colleague at 
Princeton, and after describing the problem, I might say, "You're kidding," and 
he said, "Oh, you're right. The solution is obvious, isn't it?" I don't know 
whether I would say dynamic programming or whatever, but several papers came 
out of this intense collaboration, and we got to the point where we could 
communicate with just a few words. We had a very large, shared symbol table.
\cite{aho_oral_history_2022}
\end{quotation}

\begin{quotation}
\textbf{Collaboration with Ullman}
Aho is best known for the textbooks he wrote with Ullman, his co-awardee. The 
two were full time colleagues for three years at Bell Labs, but after going 
back to Princeton as a faculty member Ullman continued to work one day a week 
for Bell.

They retained an interest in the intersection of automata theory with formal 
language. In an early paper, Aho and Ullman showed how it was possible to make 
Knuth's LR(k) parsing algorithm work with simple grammars that technically did 
not meet the requirements of an LR(k) grammar. This technique was vital to the 
Unix software tools developed by Aho and his colleagues at Bell Labs. That was 
just one of many contributions Aho and Ullman made to formal language theory 
and to the invention of efficient algorithms for lexical analysis, syntax 
analysis, code generation, and code optimization. They developed efficient 
algorithms for data-flow analysis that exploited the structure of "gotoless" 
programs, which were at the time just becoming the norm.
\cite{aho_turing_award_2020}
\end{quotation}

\begin{quotation}
\textbf{The Early History of Software, 1952-1968 101}

In the early 1960s computer science struggled to define itself and its purpose, 
in relation not only to established disciplines of electrical engineering and 
applied mathematics, but also in relation to—and as something distinct from—the 
use of computers on campus to do accounting, record keeping, and administrative 
work.58 Among those responsible for the discipline that emerged, Professor 
George Forsythe of Stanford's mathematics faculty was probably the most 
influential. With his prodding, a Division of Computer Science opened in the 
mathematics department in 1961; in 1965 Stanford established a sepa-rate 
department, one of the first in the country and still one of the most 
well-regarded.59
\cite{history_of_modern_computing_2003_ceruzzi}
\end{quotation}

\section{Bell Labs II}

\todo{Bjarne Stroustrup, C++ (1979); Dennis Ritchie, C (1972); Ken Thompson, B (1969); Brian Kernighan, AWK (1977), AMPL (1976), co-author of The C Programming Language (1978)}

\section{Commoditization}

\todo{Bill Gates and Paul Allen (Microsoft) | Microsoft BASIC (1975) | Developed the 
first critical piece of commercial software for personal computers, 
establishing the doctrine that software should be a purchased, proprietary 
commodity. Sun microsystems, each part of the company needed to sell to all the others,
reason why their compiler was paid; proprietary Unix;}

\input{chapters/software-timeline.tex}
