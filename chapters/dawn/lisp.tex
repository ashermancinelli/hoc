\section{Lisp}

Lisp was developed primarily by John McCarthy in two bouts:
most of the key ideas were developed in 1956-1958,
and in 1958-1962 the language was actually implemented and applied to artificial
intelligence.

In terms of programming language design, Lisp's fundamental data structure is the \textit{list}--
hence the name (\textit{\textbf{LIS}t \textbf{P}rocessing}).
Symbolic computing (which was still novel at the time) was implemented using lists and basic
operations on them.
Lisp \textit{programs} were also represented as data; one of the key innovations of the language
the ability manipulate Lisp programs as any other data structure, with the \texttt{eval}
function serving as the bridge between the code as data and code as actions to be performed
by a Lisp program.
This concept of self-modifying programs was to serve as the basis for ALGOL Y,
the more ambitious successor to ALGOL 60,
which was abandoned due to the eventual scope and controversy of the ALGOL X project,
or ALGOL 68 as it came to be known.

\subsection{FORTRAN Lisp Processing Language}

Lisp was born out of John McCarthy's desire for a list-oriented language
for work on an IBM 704 at Dartmouth College during a summer research project
in 1956, which was the first organized study of artificial intelligence
\cite{mccarthy_history_of_lisp_1978}.
Around this time, McCarthy was presented the list-processing programming language \textit{IPL 2}
(\textit{Information Processing Language}), written for the RAND Corporation's JOHNNIAC computer.
\footnote{The RAND Corporation was not the same company as Remington Rand/Sperry Rand, where Grace Hopper worked around this time.}
Dartmouth was soon to get access to an IBM 704 thanks to the New England Computation Center
at MIT, which IBM was in the midst of establishing.
McCarthy was to consult with a team at IBM developing a theorem-proving program for plane geometry,
and it was not clear at the time whether IBM's FORTRAN would be suitable for list-processing.

McCarthy was also independently working on artificial intelligence,
publishing his first paper in the field in 1958,
\citetitle{mccarthy_programs_with_common_sense_1958}.
This paper, also called the \textit{Advice Taker} proposal,
involved representing information in the form of sentances in a formal language,
and an accompanying program that would make inferences based on that information.
These sentences were to be structured as lists, so he naturally
needed a list-processing language to process these sentences.

McCarthy started by considering how list structures would be represented in
memory. The IBM 704 had an addressable word size of 36 bits with
a 15-bit address space, so the pointers would need to be 15 bits,
which allowed for two pointers in each word, plus 6 extra bits.
A list in Lisp was to be represented in a word like so
\footnote{This is the layout as McCarthy describes in \citetitle{mccarthy_history_of_lisp_1978},
	but \citeauthor{gelernter_flpl_1960} include a sign bit as the uppermost bit of the tag.}:

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		               & \textbf{Tag} & \textbf{Decrement}      & \textbf{Prefix} & \textbf{Address}     \\
		\hline
		Width          & 3 Bits       & 15 Bits                 & 3 Bits          & 15 Bits              \\
		\hline
		Purpose        & Type Code    & Address of Rest of List & Type            & Data/Pointer to Data \\
		\hline
		Lisp primitive & \texttt{ctr} & \texttt{cdr}            & \texttt{cpr}    & \texttt{car}         \\
		\hline
	\end{tabular}
	\caption{Layout of a 36-bit word on the IBM 704 as used for Lisp list structures.}
\end{table}

Nathaniel Rochester, Herbert Gelernter and Carl Gerberich at IBM
took on the task of writing implementing this list-processing language
in FORTRAN, called FLPL for \textit{FORTRAN-Compiled List Processing Language}.
They considered using the IPL, but decided against it, persuaded by McCarthy's suggestion
to instead adapt a FORTRAN compiler.
This primitive version of the language lacked conditional expressions, recursion, and other
fundamental features.

They note in \citetitle{gelernter_flpl_1960} that while IPL's interpretation was prohibatively
costly, their adaptation of FORTRAN was able to run the same programs in a fraction of the time.
They also pointed out the utility of FORTRAN III's introduction of separable compilation and linking of
hand-written machine code and regular FORTRAN code, which allowed them to hand-write certain
performance-critical sections of their list-processing programming environment\cite{gelernter_flpl_1960}:

\begin{quotation}
	It must be emphasized that
	FORTRAN is in itself an information processing language of great versatility and
	sophistication. Our list-processing functions merely serve to increase the "vocabulary"
	of the language so that list manipulation processes may be described within
	the FORTRAN framework as are ordinary computer processes. We are thus able
	to enjoy the same ease of programming, ease of modification, and estensive debugging
	aids available to the programmers of standard numerical problems.
\end{quotation}

Thus FLPL was not necessarily a compiler or interpreter, but a set of libraries and tools
that made list-processing within FORTRAN programming environment easier.
The numerical features of FORTRAN and the runtime libraries for I/O and formatting
were available in FLPL, but the programming environment also made available operations
to evaluate symbolic expressions and manipulate lists.
Notably, FLPL lacked the ability to treat machine code as data, a feature that IPL had
and Lisp would come to be famous for.
\citeauthor{gelernter_flpl_1960} conclude their paper by pointing this out:

\begin{quotation}
	One feature of IPL V is excluded from FLPL by the nature of a compiler.
	Sequences of IPL instructions to be interpreted are stored in the computer as
	NSS lists, just as are the data. Although this property has been largely irrelevant
	to all programs written to date, it is conceivable that one might want to write
	a program in which the symbolic entities that are processed are IPL instructions
	themselves, and in which transfers of control take place between the metaprogram and
	the machine-generated one. The fact that the transformation of
	FLPL expressions into computer activity is a two-stage, irreversible process
	places this kind of behavior beyond the range of our language, even though it is
	quite feasible to manipulate FLPL expressions within FLPL.
\end{quotation}

\subsection{McCarthy's Contributions}

Nathaniel Rochester invited McCarthy to join the IBM Information Research Department for the
summer of 1958 to implement differentiation of algebraic expressions in FLPL,
where he would go on to develop many more foundational concepts.
In fact, the differentiation program would never be completed due to limitations in FLPL,
and his time there was demarcated by pushing the boundaries of what was possible in
the language.

McCarthy developed the conditional expressions in 1957-1958 while developing a chess program
in FORTRAN, though it was not the conventional notion of an if-statement since both arms
of the conditional expression were always evaluated; \texttt{XIF(C, E1, E2)} would return
\texttt{E1} if \texttt{C} was equal to 1 and \texttt{E2} otherwise, which we would now call
a \textit{merge} or \textit{select} operation.
This was motivated by the clumsy syntax and semantics of the \texttt{IF} statement in FORTRAN I and II,
and something similar to McCarthy's version would soon be added to the language.

Because the process of differentiation is inherently recursive, McCarthy naturally
found the need for recursion in FLPL, and went about implementing it.
For differentiating an arbitrary list of symbol expressions,
he added \texttt{maplist} to apply a function to each element of a list,
the first his many steps towards higher-order programming.
To pass functions as arguments to \texttt{maplist} and other higher-order functions,
he would need a notation for functions, which he introduced with \texttt{lambda}
\cite{mccarthy_history_of_lisp_1978}:

\begin{quotation}
	use functions as arguments, one needs a notation for functions, and it seemed
	natural to use the \textlambda-notation of Church (1941).
	I didn't understand the rest of his book, so I
	wasn't tempted to try to implement his more general mechanism for defining functions.
	Church used higher order functionals instead of using conditional expressions.
	Conditional expressions are much more readily implemented on computers.
\end{quotation}

Again, we see how far Alonzo Church's \lambdacalc was ahead of the actual
implementation of programming languages capable of representing its concepts.
McCarthy also ran into the problem of deallocation of these list structures.
While list-processing made some problems far more convenient,
it also introduced new problems and some old problems more difficult.
Memory management was one of the latter.

Techniques for automatic memory management had not yet been developed,
and McCarthy would not solve this problem before his summer at IBM
came to an end.
The teams at IBM were quite pleased with the directions they were taking
FORTRAN and FLPL, dismissing recursive functions and
proper conditional expressions as not necessary.
The culmination of these limitations led
McCarthy to conclude that a new language was needed.
FORTRAN could not easily be extended because of both the complexity of the language
and existing compilers, and because of the political difficulties that
would arise from suggesting to IBM leadership that their wildly successful compiler
and programming language would need to be significantly changed.
Thus he began working on Lisp.

\subsection{Lisp is Born}

The Fall of the same year, in 1958,
McCarthy became an assistant professor at MIT, where he started
the MIT Artificial Intelligence Project with Marvin Minsky.
Their original plan was to produce a compiler, but it
was in the zeitgeist that compilers were extremely time intensive
to develop; John Backus's team noted in \citetitle{backus_etal_fortran_automatic_coding_system_1957}
that it took them 18 person-years to complete the first FORTRAN compiler,
and that was the definitive compiler for the most popular
programming language of the time.

They instead began developing a Lisp \textit{environment}, at this point a set of
functions compiled by hand for reading and printing list structures.
These programs were written in \textit{M-expressions}, an informal language resembling
FORTRAN with assignments, \texttt{goto}s, the core Lisp functions, and a few
of the features they had wanted from FORTRAN but couldn't get, namely recursive functions.

Beginning with \citetitle{mccarthy_recursive_functions_computation_1960}, McCarthy and the
team at MIT began writing papers describing theories in Lisp.
The \citeyear{mccarthy_recursive_functions_computation_1960} paper was for using Lisp
as both a programming language and formal language for recursive function theory,
and several subsequent papers represented Lisp programs in first-order logic.
A more impactful application was the attempt to prove that Lisp was more elegant than
turing machines by formally describing a \textit{universal Lisp function},
and showing that it was more elegant than the Turing machine.

This universal function would be a Lisp program capable of evaluating
any other possible Lisp program. McCarthy described it formally as $eval[e, a]$
where the function $eval$ takes a Lisp expression $e$ and evaluates it in the context
of a list of assignments to variable names $a$.
Steve Russel noticed that an implementation of this function could
serve as an interpreter for the language--he wrote this function
in assembly, and the team had an interpreter.

Similar to Backus's experience with FORTRAN, many of the decisions they made
in designing the original Lisp without much forethought became solidified
by the availability of a programming environment, and the expectation that
the code people had written in the language would continue to work.
The number 0 representing both the empty list \texttt{NIL} and the boolean value
\texttt{FALSE} is one such case.

The prefix-notation and use of parentheses (also known as \textit{S-expressions})
are another case; they had initially set out
to write Lisp programs in their M-expressions and the prefix notation syntax was simply
the internal representation of the list structures in the language.
The tasks of formally defining M-expressions and translating them into S-expressions
was never completed, and the first users of Lisp did not mind writing programs
in the compiler's internal representation, so it stuck.
The team dubbed this version of Lisp, with all its warts, Lisp 1.

With this version, the team showed that with a few primitive functions,
\texttt{quote}, \texttt{atom}, \texttt{eq}, \texttt{car}, \texttt{cdr}, \texttt{cons}, and \texttt{cond},
they could implement a \textit{total language}, or a language capable of expressing
any Lisp function.
By providing those primitives in assembly, they had bootstrapped a programming language,
at this time just an interpreter.
It had very few features (truly, only the primitives above were available),
but they would soon extend it to produce a far more usable programming language.

\subsection{Lisp 1.5}

Many of the obvious missing features of Lisp 1 were added in Lisp 1.5 (including numbers(!)).
We will not cover them exhaustively here, but McCarthy's reccolection in \cite{mccarthy_history_of_lisp_1978}
seems to be the authoratative source and the list is short.
The one we will cover is \textit{lexical scoping}.
McCarthy points to example below as the onus for introducing lexical scoping.
James Slagle of the Lisp team wrote the following function
to find a subexpression in \texttt{x} such that the predicate \texttt{p(x)} is true,
and to return \texttt{f(x)} if so.
If the search did not find such an \texttt{x}, then the function containing
the remainder of the computation taking no arguments \texttt{u} should be called and its value
returned.

\begin{lstlisting}[language=lisp,frame=single]
(define (testr x p f u)
  (cond
    [(p x) (f x)]
    [(not (pair? x)) (u)]
    [else (testr (car x) p f (lambda () (testr (car x) p f u)))]))
    ;                         Which x is this?  ^^^^^
\end{lstlisting}

The issue, pointed out in a comment in the code block, is that the \texttt{(car x)}
component of the continuation function actually used the value from the scope
\textit{where it was called} and not the scope \textit{where it was defined}.
James had expected the latter, but got the former.
McCarthy simply thought this to be a bug and assumed that Steve Russel would fix it.

Today, these two semantics for variable scoping are called \textit{lexical scoping}
and \textit{dynamic scoping}.
Lexical scoping is the semantic James wanted, and dynamic scoping is the one he got
in the original Lisp implementation.
They are so-called because in lexical scoping, the value of a variable is determined
by the literal code surrounding the variable's use--in this case, the function body
of \texttt{testr}.
Dynamic scoping is so-called because the variable is looked up from the environment
\textit{where the variable is used}. In this case, \texttt{(car x)} is looked up in the
environment where the continuation function is called, instead of where the continuation
function is defined.

This distinction was important in the evolution of ALGOL as well--
the final ALGOL 60 report does not mention these terms for scoping by name,
but their concepts are seen in the revised report, particularly
\cite[Section 4.7.3.3. Body replacement and execution]{revised_report_on_the_algorithmic_language_algol_68_1976}.
All of the early compilers were faced similar challenges--languages like ALGOL
that were designed from the top-down by a committee and languages like Lisp
that were designed from the bottom-up for a particular purpose still had to
grapple with the same issues eventually.

Russell eventually did implement a solution for James through the \texttt{FUNARG}
construct, which allowed a function definition to take with it a pointer to the
environment where it was defined, thereby permitting lexical scoping in a language
with dynamic scoping.
Members of the team at MIT discussed solutions to this problem in
\citetitlecite{weizenbaum_funarg_explained_1968}
and
\citetitlecite{moses_function_of_function_1970}
a few years later in 1968 and 1970, respectively.

Along with the problem of lexical and dynamic scoping, the team also provided a means for
\gls{call-by-name}, very similar to the concept in ALGOL.
The "functions" \texttt{FEXPRS} and \texttt{FSUBRS} are not given regular arguments, but are given expressions
that will evaluate to their arguments.
The function bodies can use Lisp's \texttt{eval} function to actually evaluate their arguments in the body of the
function as they see fit.
While this is similar to ALGOL's \gls{call-by-name} which treated all unquoted arguments this way,
the Lisp versions allowed users to decided exactly when and how their arguments were evaluated.
Users could pass the code that comprised their arguments to further calls to
\texttt{FEXPRS} and \texttt{FSUBRS}, or they could evaluate some of them at the start
and some of them later, and so on.

\subsection{The First Lisp Compilers}

\citeauthor{allen_anatomy_of_lisp_1978} collected the early histories of Lisp compilers
in \cite{allen_anatomy_of_lisp_1978}
from a few primary sources that are difficult to find, namely
the paper \citetitle{blair_structure_lisp_compiler_1971} by \citeauthor{blair_structure_lisp_compiler_1971},
which was apparently never published.
Essentially, a few different teams set out to develop compilers for Lisp
to varying degrees of success and completion.

The first was Robert Brayton at MIT \cite{blair_structure_lisp_compiler_1971},
who started working on the first Lisp compiler in 1957 on an IBM 704,
though he was not successful until 1960, after which he left MIT.
He wrote this compiler in assembly, and so it was thought that his compiler
would be more performant than the competing Lisp compilers, as they were
all written in Lisp themselves.
Evidently, nobody was able (or willing, perhaps) to maintain this compiler
after Brayton left MIT.

Klim Maling started work on a Lisp compiler written in Lisp
in the time after Brayton began work on his compiler, but before the compiler
was functional, though the project was dropped before it was functional itself.

Timothy Hart and Michael Levin developed the first complete Lisp compiler,
also written in Lisp, which
they claimed was the first \gls{bootstrap}ped compiler,
or the first compiler to be written in the language being compiled.
This compiler was distributed with the Lisp 1.5 system for the IBM 704,
and was the basis for much of the future work on Lisp compilers.
While future versions of Lisp were being worked on, most users in this period
were stuck with this Lisp 1.5 developed at MIT, which fixed only
the most glaring of the issues with the original language.

As \citeauthor{blair_structure_lisp_compiler_1971} recalls in \citetitle{blair_structure_lisp_compiler_1971},
the first Lisp compilers were quite straightforward compared to the contemporary compilation efforts
with ALGOL and Fortran.
Blair points out that because Lisp is comprised only of untyped data objects,
the compiler's primary concern is that of variable binding and variable evaluation.
Blair cites Peter Landin as influential in his understanding of expression
evaluation semantics, and prefers the model he describes in \citetitle{landin_eval_of_expressions_1964}
over the typical understanding of a Lisp interpreter $eval[e, a]$, which we discussed above.

\subsection{After Lisp 1.5}

In the early 1960s, prior to Lisp 2, the language and all its implementations were still
extremely limited.
Numerical programs were painfully slow compared to other compiled languages at
the time (\textit{numbers} were only just added in the first version), the I/O
facilities were limited, and the memory model was not yet robust.
The garbage collector was implemented, but slow and limited.
Lisp objects could not be represented in registers and garbage-collected.

Two companies, Systems Development Corporation and Information International Inc.,
collaborated on the second version of Lisp with a new compiler
built for the military version of IBM's 7090 called the Q32, but system's the memory constraints
proved too restrictive.
The breakdown of the collaboration between SDC and III and MIT and Stanford along
with practical, political, and economic decisions that went into deciding which machine
the new compiler should be developed for, all added up to the abandonment of the project.
There were few subsequent efforts to radically change Lisp in the decades to follow
after the failure of this more ambitious effort.
Suffice it to say that the dominant system for AI work would become the DEC PDP-10, the
successor to the PDP-6, and the language would not look radically different from
version 1.5.

Outside of MIT, the adoption of Lisp was greatest at DEC.
Their implementations of Lisp on the PDP-6 and PDP-10 were widely used and their
instruction set was defined with Lisp in mind\cite{mccarthy_history_of_lisp_1978}.
The IBM 704 Lisp was extended to the 7090, 360, and 370.
