\section{GNU}
\label{sec:gnu}

In the 1980s, business models centered on software had grown substantially
and the Unix programming environment began permeating the industry,
bringing the C programming language along with it.
The commercialization of software has obvious advantages and disadvantages:
it is convenient that people can make a living off of writing software,
but it often stifles the collaborative environment that compilers
and programming languages grew out of.
The reaction to the commercialization of software was strong and
wiped out many companies and teams within companies dedicated to
compiler development while centering the industry around a standard,
free set of tools.

\subsection{Richard Stallman}

Richard Stallman attended Harvard in 1970, pursuing a bachelor's degree in physics.
At the end of his first year there, he began attending the MIT Artificial Intelligence Lab,
where Lisp was developed (\cref{sec:lisp}).
After graduating from Harvard in 1974, he joined the MIT lab as a graduate student.
This only lasted for a year, at which point he dropped out and began
working at the lab full-time \parencite{gross_stallman_interview_1999}.

Shortly after he began working full-time, many of the original hackers that
drew him to MIT began starting their own companies
and hiring away the other hackers at MIT.
The two most significant companies were Symbolics and \textit{Lisp Machines, Inc.} (LMI),
both set out to build \gls{lisp-machine}s and Lisp-based operating systems
based on their founder's experience working with Lisp at MIT.

Both companies attempted some level of collaboration with MIT,
to the extent that Symbolics and LMI both \textit{continued to use MIT's machines
	and source code}, which led to intellectual property disagreements.
Symbolics eventually reached an agreement with MIT such that MIT would continue
to benefit from Symbolic's Lisp machine developments \textit{only as users},
and members of MIT were not allowed to view Symbolic's source code.

In an effort to maintain MIT's hacking culture, Stallman sought to replicate
\footnote{Initially, Stallman read Symbolics' source code instead of
	replicating their features from scratch;
	\citeauthor{weinreb_symbolics_history_2020} understood Stallman's prior
	statements to mean that he did not read Symbolics' source code,
	which he pointed out in \parencite{weinreb_symbolics_history_2020},
	and Stallman left a footnote in
	\parencite[footnote 7]{stallman_my_lisp_experiences_2002}
	noting that Symbolics' source code was made available to MIT and he re-implemented
	their features before eventually deciding against even reading it.}
Symbolics' Lisp machine developments as they came out, such that the software
available to MIT would keep feature parity with that of the commercial products.
This benefited LMI, since they were allowed to see and use MIT's Lisp code.
Stallman \footnote{Again, this is Stallman's word against Weinreb's.
	There are many conflicting accounts.} kept LMI in business out of spite
for Symbolics, because he percieved them to have taken away his community.

\subsection{Birth of GNU and GCC}

Stallman tired of attempting to punish Symbolics and began considering
what he should do next.
By mid-1983, Stallman had considered building a new community around
a free operating system similar to Unix, GNU, for \textit{GNU's Not Unix}.
He no longer worked for MIT, but he was still able to use their systems.

While the first and primary program he worked on for GNU
was the GNU Emacs editor (which he rewrote after developing the original Emacs
at MIT, which he took over from Guy Steele
\parencite[footnote 1]{stallman_my_lisp_experiences_2002}),
the GNU C compiler followed quickly.

\citeauthor{von_hagen_definitive_guide_gcc_2011} notes that the idea for GCC
actually predates the wider GNU project:

\begin{quotation}
	In late 1983, just before he started
	the GNU Project, Richard M. Stallman, president of the Free Software Foundation and originator of the
	GNU Project, heard about a compiler named the Free University Compiler Kit (known as VUCK) that
	was designed to compile multiple languages, including C, and to support multiple target CPUs. Stallman
	realized that he needed to be able to bootstrap the GNU system and that a compiler was the first
	strap he needed to boot. So he wrote to VUCK’s author asking if GNU could use it. Evidently, VUCK’s
	developer was uncooperative, responding that the university was free but that the compiler was not.
	As a result, Stallman concluded that his first program for the GNU Project would be a multilanguage,
	cross-platform compiler.
	\parencite{von_hagen_definitive_guide_gcc_2011}
\end{quotation}

GCC originally stood for the GNU C Compiler, but as the GNU project grew,
new frontends were added for other languages, thus GCC now stands for the GNU Compiler Collection.
There were two primary software projects that Stallman based the original GCC on:
the Pastel compiler, and the Portable Optimizer.

\subsubsection{The Pastel Compiler}

Stallman recalls in \parencite{stallman_the_gnu_project} that he thereafter obtained
the code for a cross-platform compiler from Lawrence Livermore National Laboratory (LLNL)
called \textit{Pastel}, because it compiled (and was written in) an "off-color Pascal,"
as the authors described it to Stallman \parencite{stallman_kth_transcript_1986}.

Pastel was the compiler and programming language of choice for the Amber
operating system (a competitor to Multics)
being developed at LLNL for the S-1 computer (a competitor to the Cray-1).
This OS was originally written in PL/1, but the language was found to be
inadequate; the Amber developers originally extended PL/1 with macros
to improve the type definition system, but after struggling for about 6 months
with the limitations
of PL/1's type system and module system, and the high financial cost
of the G PL/1 compiler, LLNL decided to use another language
\parencite{frankston_amber_os_bach_thesis_pastel_compiler_mit_1984}.

Jeff Broughton, the leader of the Amber project, wrote two compilers
for Pascal, extending the language as he saw fit. The first of these
compilers was completed in a few months, and targetted PL/1 instead of
S-1 machine code so the compiler could be used while the native backend
was still being developed.
The existing PL/1 codebase was ported to Pastel over the course of a few months.
It was around this time that Stallman visited LLNL and learned of the Pastel compiler.
A historical note from 1998 in \citeauthor{frankston_amber_os_bach_thesis_pastel_compiler_mit_1984}'s
Bachelor's thesis makes note of this interaction
\parencite{frankston_amber_os_bach_thesis_pastel_compiler_mit_1984}:

\begin{quotation}
	[A]t one point in the project Richard Stallman visited, and
	had the Pastel compiler explained to him.
	He left with a copy of the source, and used it to produce the Gnu C compiler.
	Most of the techniques that gave the Gnu C compiler its reputation
	for good code generation came from the Amber Pastel compiler.
\end{quotation}

Pastel was an interesting language and compiler in its own right.
The compiler supported parameterized types and complicated features
handled by template metaprogramming in C++.
Stallman points to the example of strings; in Pastel, a programmer
could specify a \texttt{string} if they wanted a dynamically sized string,
or \texttt{string(n)} if they knew the length ahead of time.
The compiler could then store the string in static memory and reuse it
with each function call, saving an allocation and deallocation.

He began to add a C frontend and a Motorola 68000 backend to this compiler,
but he ran into some issues stemming from the extended Pascal it was
written in.
This version of Pascal did not require users to forward-declare functions,
meaning the compiler had to process the entire file to find the declarations
for every function all at once, consuming an amount of memory proportional
to the size of the file being compiled.
Stallman was working on what he called "a horrible version of Unix"
\parencite{stallman_kth_transcript_1986} with
needlessly limited stack space, which further complicated the use of this compiler.
When he attempted to perform liveness analysis of temporary values
(where the compiler determines which values are used by the program in a given region)
he needed a quadratic matrix of bits, which took up to several hundred kilobytes
for large functions.

\subsubsection{The Portable Optimizer}

Stallman also took inspiration from the University of Arizona
Portable Optimizer
especially in store-to-load \gls{forward}ing and \gls{strength-reduction}.
This optimizer was also called PO, which stands for \textit{peephole optimizer}
and not \textit{portable optimizer}.
In this USENET discussion, \citeauthor{fraser_register_transfer_language_gcc_usenet_1990}
responds to a question about why GCC uses \acrfull{rtl} for an intermediate representation
instead of simple Lisp tuples, connecting GCC to its history with PO:

\begin{quotation}
	GCC is based in part on PO, a retargetable peephole optimizer that Jack
	Davidson and I developed at the University of Arizona starting in 1978\dots

	Most peephole optimizers operate on machine instructions, and they need to
	know what the instructions do. A machine-specific peephole optimizer can
	use a machine-specific representation for instructions, and the programmer
	can burn into the optimizer machine-specific information about the effects
	of instructions. A retargetable peephole optimizer, however, needs a
	machine-independent way to represent the effects of machine-specific
	instructions. (If that sounds like a contradiction, consider C: one can
	write machine-specific C programs, but the language itself is machine
	independent.) Register transfers are such a representation.
	\parencite{fraser_register_transfer_language_gcc_usenet_1990}
\end{quotation}

\acrshort{rtl} is discussed at length in \parencite{davidson_regalloc_peephole_ops_rtl_1984}.
GCC continues to use \acrshort{rtl} in its backend, as well as machine-specific
peephole optimizations initially developed by Jack Davidson and Chris Fraser.

\todo{review content in \parencite{smotherman_s1_supercomputer_1975_1988}.}

\subsection{GDB}

Based on DBX debugger.

\subsection{The Kernel}

Linux was started by Linus Torvalds after Stallman began work on GNU,
so as far as Stallman knew, he still needed to come up with a kernel
to provide a complete, free operating system capable of competing with Unix.
The GNU community considered two existing projects for the GNU operating system kernel
before arriving at Linux:
in \citeyear{stallman_kth_transcript_1986}, Stallman planned on using Trix,
which was a research project from MIT; by 1990, he planned to build a Unix-compatible
OS layer called GNU Hurd
\footnote{Initially called \textit{Alix}
	after Stallman's then-girlfriend, a system administrator at MIT.}
on top of the Mach microkernel,
designed at Carnegie Mellon University (and later at the University of Utah)
\parencite{stallman_the_gnu_project}.

Linux Torvalds fortunately started developing Linux in 1991,
and released it as free software in 1992, permitting the combination
of GNU and Linux.
Linux, compiled with GCC and paired with the other GNU utilities
formed the Unix-compatible free operating system
(and the community around it) that Stallman had sought after.


\subsection{TODO: how gnu killed lots of small teams}
% ===================

There were loads of small companies that made money leasing out their compilers,
and GNU put them all out of business essentially overnight.
Every hardware company wrote their own compilers, but once it became clear
that just adding a backend for the GNU compiler would be significantly easier
than writing a new compiler from scratch, many of the hundreds of small compiler
companies went out of business, and the compiler teams inside large companies
began using and contributing to the GNU compilers.

In \cref{chap:software}, we discussed how the C programming language permeated
the software industry via Unix.
Hardware vendors would then develop their own C compilers so their users
could use the programming language they were familiar with, similar to how
hardware vendors were expected to provide Fortran compilers.
This led to a diverse ecosystem of C compilers from different vendors targeting
their respective hardware with an unfortunately diverse set of supported language features.
This meant users could not rely on a consistent experience with C compilers when
developing software for different hardware platforms.

The \textit{automake} and \textit{autoconf} tools were developed to address this
issue with macros, but buy-and-large, this experience was unpleasant for users.
